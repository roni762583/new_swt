Hereâ€™s a complete report with implementation instructions, specs, and a **ready-to-use CPU/GPU compatible setup** for running PPO with Gymnasium and a custom trading environment inside Docker.

---

# ðŸ“„ Report: PPO + Gymnasium Custom Trading Environment in Docker

## 1. Abstract

This document describes how to implement a reproducible reinforcement learning pipeline for trading using **Proximal Policy Optimization (PPO)** with **Gymnasium** environments. It includes a **Dockerized setup** that works on both CPU-only and GPU-enabled machines, ensuring portability between local development and cloud training.

---

## 2. Architecture Overview

* **Frameworks**:

  * [Gymnasium](https://github.com/Farama-Foundation/Gymnasium) â€“ RL environment API.
  * [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) â€“ PPO implementation.
  * [PyTorch](https://pytorch.org/) â€“ backend for PPO model.

* **Docker**:

  * Base image supports CUDA (for GPU) but is also usable on CPU.
  * Container bundles dependencies and training scripts for reproducibility.

* **TradingEnv**:

  * Custom subclass of `gymnasium.Env`.
  * Defines `step()`, `reset()`, action/observation spaces, and reward logic.

---

## 3. Implementation Instructions

### 3.1 Dockerfile

```dockerfile
# Base: works for both CPU and GPU
FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04

# System deps
RUN apt-get update && apt-get install -y \
    python3 python3-pip git && \
    rm -rf /var/lib/apt/lists/*

# Upgrade pip
RUN pip3 install --upgrade pip

# Install Python deps
RUN pip3 install \
    gymnasium \
    stable-baselines3[extra] \
    cleanrl \
    numpy pandas matplotlib

# Optional: PyTorch + TorchRL (CPU or GPU auto-selected)
RUN pip3 install torch torchvision torchaudio torchrl --index-url https://download.pytorch.org/whl/cu121

# Working directory
WORKDIR /app

# Copy code
COPY . /app

# Default entrypoint
CMD ["python3", "train.py"]
```

ðŸ‘‰ For **CPU-only machines**, you may replace the base image with:

```dockerfile
FROM python:3.11-slim
```

---

### 3.2 Example `train.py`

```python
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
from trading_env import TradingEnv  # custom env

# Create env
env = TradingEnv()
check_env(env)  # verify compliance

# Define PPO agent
model = PPO(
    policy="MlpPolicy",
    env=env,
    verbose=1,
    tensorboard_log="./tensorboard/"
)

# Train
model.learn(total_timesteps=200_000)

# Save model
model.save("ppo_trading")
env.close()
```

---

### 3.3 Example `trading_env.py` (minimal template)

```python
import gymnasium as gym
from gymnasium import spaces
import numpy as np

class TradingEnv(gym.Env):
    metadata = {"render_modes": ["human"]}

    def __init__(self):
        super().__init__()
        # Example: actions = [0=hold, 1=buy, 2=sell]
        self.action_space = spaces.Discrete(3)
        # Observation: [price, position, cash]
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(3,), dtype=np.float32
        )
        self.reset()

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.price = 100.0
        self.position = 0
        self.cash = 1000
        obs = np.array([self.price, self.position, self.cash], dtype=np.float32)
        return obs, {}

    def step(self, action):
        # Dummy price move
        self.price += np.random.randn()

        if action == 1:  # buy
            self.position += 1
            self.cash -= self.price
        elif action == 2:  # sell
            self.position -= 1
            self.cash += self.price

        reward = self.cash + self.position * self.price
        obs = np.array([self.price, self.position, self.cash], dtype=np.float32)
        terminated = False
        truncated = False
        return obs, reward, terminated, truncated, {}

    def render(self):
        print(f"Price: {self.price:.2f}, Pos: {self.position}, Cash: {self.cash}")
```

---

## 4. Running Instructions

### 4.1 Build

```bash
docker build -t ppo-trading .
```

### 4.2 Run on CPU

```bash
docker run -it --rm ppo-trading
```

### 4.3 Run on GPU

```bash
docker run --gpus all -it --rm ppo-trading
```

### 4.4 Save Models

```bash
docker run -it --rm \
    -v $(pwd)/models:/app/models \
    ppo-trading
```

---

## 5. Key Considerations

1. **Custom Env**

   * Implement `reset()` and `step()` correctly.
   * Reward shaping = critical (e.g., returns, Sharpe ratio).
   * Observations should include features like OHLCV, indicators, position, cash.

2. **Training Stability**

   * Use `tensorboard` logs (`tensorboard --logdir=./tensorboard`).
   * Tune PPO hyperparameters (learning rate, clip range).

3. **Scaling**

   * Use `SubprocVecEnv` for multiple parallel envs if you need throughput.
   * Environment must be pickleable for multiprocessing.

---

## 6. Conclusion

This system provides a **reproducible PPO training pipeline** for trading research:

* Runs identically on CPU or GPU.
* Uses Docker for clean deployment.
* Supports Gymnasium-compliant custom trading environments.
* Easily extensible with other SB3 algorithms (A2C, DDPG, etc.).

---

