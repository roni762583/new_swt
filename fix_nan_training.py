#!/usr/bin/env python3
"""
Fix NaN Training Issues

The model is experiencing NaN losses due to unstable gradients.
This script fixes:
1. Too high learning rate causing gradient explosion
2. Missing gradient safeguards
3. Unstable value distributions
"""

import re


def fix_training_stability():
    """Fix NaN training issues by stabilizing learning parameters."""

    print("üîß Fixing NaN training stability issues...")

    # Fix 1: Reduce initial learning rate to prevent gradient explosion
    with open("/home/aharon/projects/new_swt/micro/training/train_micro_muzero.py", "r") as f:
        content = f.read()

    # Lower the learning rate from 5e-4 to 2e-4 (more stable)
    content = re.sub(
        r'initial_lr: float = 5e-4.*?# Higher initial learning rate',
        'initial_lr: float = 2e-4  # Stable learning rate to prevent NaN',
        content
    )

    # Reduce gradient clipping for more aggressive clipping
    content = re.sub(
        r'gradient_clip: float = 10\.0',
        'gradient_clip: float = 5.0  # More aggressive clipping',
        content
    )

    with open("/home/aharon/projects/new_swt/micro/training/train_micro_muzero.py", "w") as f:
        f.write(content)

    print("‚úÖ Fixed learning rate and gradient clipping")

    # Fix 2: Add NaN detection and recovery in value network
    print("üîß Adding NaN detection to value network...")

    value_net_path = "/home/aharon/projects/new_swt/micro/models/micro_networks.py"
    with open(value_net_path, "r") as f:
        content = f.read()

    # Add NaN safeguards to value network forward
    if "torch.isnan(logits).any()" not in content:
        # Find the value network forward method
        value_forward_pattern = r'(class ValueNetwork.*?def forward\(self, hidden_state: torch\.Tensor\) -> torch\.Tensor:.*?logits = self\.head\(x\))(.*?return logits)'

        def add_nan_check(match):
            before = match.group(1)
            after = match.group(2)
            nan_check = """

        # Critical: Prevent NaN propagation
        if torch.isnan(logits).any():
            logger.error("NaN detected in value logits - resetting to uniform distribution")
            logits = torch.zeros_like(logits)
            # Set middle value (0 change) to highest probability
            middle_idx = logits.size(-1) // 2
            logits[..., middle_idx] = 1.0"""
            return before + nan_check + after

        content = re.sub(value_forward_pattern, add_nan_check, content, flags=re.DOTALL)

    with open(value_net_path, "w") as f:
        f.write(content)

    print("‚úÖ Added NaN detection to value network")

    # Fix 3: Add loss scaling and NaN recovery to training loop
    print("üîß Adding NaN recovery to training loop...")

    train_path = "/home/aharon/projects/new_swt/micro/training/train_micro_muzero.py"
    with open(train_path, "r") as f:
        content = f.read()

    # Add NaN detection after loss calculation
    if "torch.isnan(total_loss)" not in content:
        loss_pattern = r'(total_loss = policy_loss \+ value_loss \+ reward_loss)(.*?total_loss\.backward\(\))'

        def add_loss_check(match):
            before = match.group(1)
            after = match.group(2)
            nan_check = """

            # Critical: Check for NaN loss and skip if detected
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                logger.error(f"NaN/Inf loss detected: {total_loss.item():.6f} - skipping this batch")
                self.optimizer.zero_grad()
                continue"""
            return before + nan_check + after

        content = re.sub(loss_pattern, add_loss_check, content, flags=re.DOTALL)

    with open(train_path, "w") as f:
        f.write(content)

    print("‚úÖ Added NaN recovery to training loop")

    # Fix 4: Start fresh without corrupted checkpoint
    print("üîß Removing potentially corrupted checkpoint...")

    import os
    checkpoint_path = "/home/aharon/projects/new_swt/micro/checkpoints/latest.pth"
    if os.path.exists(checkpoint_path):
        os.remove(checkpoint_path)
        print("‚úÖ Removed corrupted checkpoint - will start fresh")
    else:
        print("‚ÑπÔ∏è No existing checkpoint found")

    print("\nüéØ STABILITY FIXES APPLIED:")
    print("‚úÖ Reduced learning rate: 5e-4 ‚Üí 2e-4")
    print("‚úÖ Aggressive gradient clipping: 10.0 ‚Üí 5.0")
    print("‚úÖ Added NaN detection in value network")
    print("‚úÖ Added NaN recovery in training loop")
    print("‚úÖ Removed corrupted checkpoint")
    print("\nüí° Training should now be stable without NaN losses!")


if __name__ == "__main__":
    fix_training_stability()