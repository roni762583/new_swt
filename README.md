# ğŸš€ PPO Trading System - GBPJPY Forex Implementation
**Version 3.1 | Last Updated: September 29, 2025 | Active Training**

## ğŸ”´ ACTIVE SYSTEM: PPO (Proximal Policy Optimization)
**Location**: `/micro/nano/picco-ppo/` | **Container**: `ppo-training` (running)

### Current Training Status (Live - Improved Version)
- **Version**: v2 with rolling Ïƒ gating and weighted learning
- **Timesteps**: 16,384+ (target: 1,000,000)
- **Win Rate**: ~11-13% (improving from 6%)
- **Training Phase**: Weighted learning (winners: 1.0, losers: 0.2â†’1.0)
- **Architecture**: PPO with 128Ã—128 MLP (reduced for less overfitting)
- **Gating**: 56% gate rate (successfully filtering noise)

### Performance Metrics
- **Win Rate Trend**: Stable at 12-13% throughout training
- **Average Loss**: -0.5 to -2.0 pips (after 4 pip spread)
- **Average Win**: +0.2 to +4.7 pips (occasional)
- **Trading Speed**: ~5,000 trades/hour
- **Target**: 1,000,000 timesteps for full training

## ğŸ“Š System Overview

A **production-grade PPO implementation** for forex trading (GBPJPY M5/H1) using 17 carefully selected features with AMDDP1 reward function. The system overcomes a 4 pip spread handicap through sophisticated feature engineering and phased learning.

### Key Features
- **PPO Algorithm**: Proximal Policy Optimization with stable-baselines3
- **Feature Engineering**: 7 market + 6 position + 4 time features
- **Phased Learning**:
  - Phase 1: Learn from winners only (first 1000 profitable trades)
  - Phase 2: Normal learning from all trades (current)
- **Reward Function**: AMDDP1 (pips - 0.01Ã—drawdown)
- **Trading Costs**: 4 pip fixed spread on position opening

## ğŸ“ˆ Performance Analysis

### Win Rate Evolution
| Milestone | Win Rate | Profitable/Total |
|-----------|----------|------------------|
| 5,000 trades | 12-13.5% | ~650/5,000 |
| 10,000 trades | 12.15% | 1,215/10,000 |
| 15,000 trades | 12.7% | 1,908/15,000 |
| 20,000 trades | 12.9% | 2,581/20,000 |
| 25,000 trades | 12.69% | 3,173/25,000 |
| **27,400 trades** | **12.67%** | **3,472/27,400** |

### Expected Training Timeline
- **Current Progress**: 27,400 trades in ~5.5 hours
- **Training Speed**: ~5,000 trades/hour
- **To Breakeven**: Expected at 50,000-100,000 trades
- **To Profitability**: Expected at 100,000+ trades
- **Full Training**: 1,000,000 timesteps target

---

## ğŸ³ Quick Start - PPO Training

```bash
# Navigate to PPO directory
cd /home/aharon/projects/new_swt/micro/nano/picco-ppo/

# Check training status
docker logs ppo-training --tail 50

# Monitor live performance
docker exec ppo-training cat results/latest.json

# Stop training
docker stop ppo-training
```

---

## ğŸ—ï¸ System Architectures

### Active: PPO Implementation (`/micro/nano/picco-ppo/`)
- **Algorithm**: Proximal Policy Optimization (stable-baselines3)
- **Network**: 256Ã—256 MLP with ReLU activation
- **Features**: 17 dimensions (market + position + time)
- **Container**: Single training container with checkpointing

### Legacy: MuZero Implementation (`/micro/`)
- **Algorithm**: Stochastic MuZero with MCTS
- **Architecture**: TCN + 5 neural networks
- **Features**: 15 dimensions (temporal + static)
- **Status**: Not currently active

---

## ğŸ“ Project Structure

```
new_swt/
â”œâ”€â”€ README.md                    # This document
â”œâ”€â”€ micro/                       # MuZero implementation (legacy)
â”‚   â”œâ”€â”€ nano/                    # Analysis tools
â”‚   â”‚   â””â”€â”€ picco-ppo/          # ğŸ”´ ACTIVE PPO system
â”‚   â”‚       â”œâ”€â”€ env/            # Trading environment
â”‚   â”‚       â”œâ”€â”€ train.py        # Main training script
â”‚   â”‚       â”œâ”€â”€ config.py       # Centralized configuration
â”‚   â”‚       â””â”€â”€ checkpoints/    # Model saves
â”‚   â””â”€â”€ training/               # MuZero training (inactive)
â””â”€â”€ data/                       # GBPJPY market data
```

---

## ğŸ“Š Detailed MuZero Architecture (Reference)

#### Model Architecture: Stochastic MuZero with TCN
```python
# Five Core Networks
1. Representation Network (observation â†’ hidden state)
   - Input: Temporal (32, 9) + Static (6,)
   - TCN Block: 9 â†’ 240 channels (3 residual blocks)
   - Static MLP: 6 â†’ 16 dimensions
   - Output: 256-dimensional hidden state

2. Dynamics Network (state transition)
   - Input: hidden_state (256) + action (4) + outcome (3)
   - Architecture: 3 MLPResidual blocks
   - Output: next_state (256) + reward prediction

3. Policy Network (action prediction)
   - Input: hidden_state (256)
   - Architecture: 2 MLPResidual blocks â†’ 4 actions
   - Output: action_logits with temperature scaling

4. Value Network (state evaluation)
   - Input: hidden_state (256)
   - Support: [-300, +300] pips (601 bins)
   - Output: categorical distribution over value bins

5. Afterstate Network (deterministic transition)
   - Input: hidden_state (256) + action (4)
   - Output: afterstate (256) before stochastic outcome
```

#### 1.2 TCN (Temporal Convolutional Network) Block
```python
class TCNBlock:
    in_channels: 9 (temporal features)
    out_channels: 240
    kernel_size: 3
    num_layers: 3
    dilation: [1, 2, 4]  # Exponential receptive field
    dropout: 0.1
    activation: ReLU
    batch_norm: True
```

#### 1.3 Training Hyperparameters
```python
# Core Training Config
learning_rate: 0.002 (fixed, no decay)
batch_size: 64
buffer_size: 10,000
num_simulations: 5 (MCTS per step)
episode_length: 360 bars (6 hours)
target_episodes: 1,000,000
checkpoint_interval: 50 episodes

# Optimizer
optimizer: Adam
weight_decay: 1e-5
gradient_clip: 1.0

# Exploration
epsilon: 0.2 (epsilon-greedy)
temperature: 1.0 â†’ 0.5 (decay over 20k episodes)
dirichlet_alpha: 1.0
dirichlet_fraction: 0.5

# Loss Weights
value_loss_weight: 0.25
policy_loss_weight: 1.0
reward_loss_weight: 1.0
outcome_loss_weight: 1.0
```

### 2. Feature Engineering Pipeline

#### 2.1 Input Features (15 total dimensions)
```python
# Temporal Features (32 timesteps Ã— 9 features)
Market Features (indices 0-4):
  0: position_in_range_60     # (close - min60) / (max60 - min60)
  1: min_max_scaled_momentum_60  # 60-bar momentum, normalized [0,1]
  2: min_max_scaled_rolling_range  # 60-bar volatility, normalized
  3: min_max_scaled_momentum_5   # 5-bar momentum, normalized
  4: price_change_pips         # tanh((close[t] - close[t-1]) * 100 / 10)

Time Features (indices 5-8):
  5: dow_cos_final   # cos(2Ï€ Ã— day_of_week / 5)
  6: dow_sin_final   # sin(2Ï€ Ã— day_of_week / 5)
  7: hour_cos_final  # cos(2Ï€ Ã— hour / 120) for 120hr trading week
  8: hour_sin_final  # sin(2Ï€ Ã— hour / 120)

# Static Features (1 timestep Ã— 6 features)
Position Features (indices 9-14):
  9:  position_side      # -1 (short), 0 (flat), 1 (long)
  10: position_pips      # tanh(current_pips / 100)
  11: bars_since_entry   # tanh(bars / 100)
  12: pips_from_peak     # tanh(drawdown_pips / 100)
  13: max_drawdown_pips  # tanh(max_dd / 100)
  14: accumulated_dd     # tanh(area_under_dd / 100)
```

#### 2.2 Market Outcome Discretization
```python
# Three discrete outcomes based on rolling Ïƒ
rolling_std = 20-period standard deviation
outcome_threshold = 0.33 * rolling_std

Outcomes:
  0: DOWN  (price_change < -threshold)
  1: NEUTRAL (-threshold â‰¤ price_change â‰¤ threshold)
  2: UP    (price_change > threshold)
```

### 3. Data Pipeline

#### 3.1 Data Sources
```yaml
Primary Data:
  File: data/GBPJPY_M1_REAL_2022-2025.csv
  Format: OHLCV 1-minute bars
  Period: Jan 2022 - Aug 2025 (3.59 years)
  Total Bars: 1,333,912
  Quality Metrics:
    - Mean pip range: 3.87 pips/min
    - 99th percentile: 16.47 pips
    - Coverage: 94.9% of trading hours

Feature Database:
  File: data/micro_features.duckdb
  Tables: features_micro
  Columns: 15 (as specified above)
  Indexing: bar_index (primary key)
```

#### 3.2 Session Sampling
```python
# Valid Trading Session Criteria
session_length: 360 bars (6 hours)
max_gap: 10 minutes
exclude_weekends: True
min_liquidity: 1.0 pip range

# Pre-indexing at startup
valid_sessions = []
for i in range(len(data) - 360):
    if validate_session(data[i:i+360]):
        valid_sessions.append(i)
# Result: ~5,000 valid sessions from 1.33M bars
```

### 4. Trading Environment

#### 4.1 Action Space
```python
Actions = {
    0: HOLD,   # No position change
    1: BUY,    # Enter long position
    2: SELL,   # Enter short position
    3: CLOSE   # Exit current position
}

# Position constraints
max_position: 1 unit
position_type: net (no hedging)
```

#### 4.2 Reward System
```python
# AMDDP1 (Average Maximum Drawdown with 1% Penalty)
def calculate_reward(action, position, pnl):
    if action in [BUY, SELL]:
        return +1.0  # Entry reward
    elif action == HOLD:
        if position == 0:
            return -0.05  # Idle penalty
        else:
            return 0.0   # In-trade hold
    elif action == CLOSE:
        amddp = pnl - 0.01 * max_drawdown
        return amddp
```

#### 4.3 Transaction Costs
```python
spread: 4 pips (fixed)
commission: 0
slippage: 0 (using limit orders)
margin: Not modeled (assumed sufficient)
```

---

## ğŸ³ Docker Deployment Architecture

### Container Specifications

```yaml
# docker-compose.yml structure
version: '3.8'

services:
  micro-training:
    image: micro-muzero:latest
    container_name: micro_training
    resources:
      memory: 8GB
      cpus: 6.0
    volumes:
      - ./micro:/workspace/micro
      - ./data:/workspace/data:ro
      - ./micro/checkpoints:/workspace/micro/checkpoints
    environment:
      - PYTHONPATH=/workspace
      - OMP_NUM_THREADS=1
    command: python3 /workspace/micro/training/train_micro_muzero.py

  micro-validation:
    image: micro-muzero:latest
    container_name: micro_validation
    resources:
      memory: 4GB
      cpus: 2.0
    volumes:
      - ./micro:/workspace/micro
      - ./data:/workspace/data:ro
      - ./micro/checkpoints:/workspace/micro/checkpoints
      - ./micro/validation_results:/workspace/micro/validation_results
    command: python3 /workspace/micro/validation/validate_micro_watcher.py
    environment:
      - MC_RUNS=200  # Reduced from 1000
      - VALIDATION_TIMEOUT=1800

  micro-live:
    image: micro-muzero:latest
    container_name: micro_live
    resources:
      memory: 2GB
      cpus: 1.0
    volumes:
      - ./micro:/workspace/micro
      - ./data:/workspace/data:ro
      - ./micro/checkpoints:/workspace/micro/checkpoints
    command: python3 /workspace/micro/live/trade_live.py
    environment:
      - OANDA_TOKEN=${OANDA_TOKEN}
      - OANDA_ACCOUNT=${OANDA_ACCOUNT}
```

---

## ğŸ“ˆ Current Operational Status (As of September 22, 2025)

### Training Progress
```
Episode: 4,732 / 1,000,000 (0.47% complete)
Training Speed: ~1,200 episodes/hour
Time Running: 4+ hours
Next Milestone: Episode 5,000 (ETA: 13 minutes)

Performance Metrics:
- Expectancy: -4.06 pips (still negative)
- Win Rate: 9.4%
- Trade Ratio: 75.3% (excellent engagement)
- Loss: 88.59 (decreasing slowly)

Action Distribution:
- HOLD: 25%
- BUY: 23%
- SELL: 24%
- CLOSE: 25%
```

### Known Issues & Solutions

#### Issue 1: Validation Timeout
**Problem**: Validation times out after 10 minutes with 1000 Monte Carlo runs
**Solution**: Already fixed in code but container needs restart
```bash
# Fix: Restart validation with new settings
docker restart micro_validation
```

#### Issue 2: Training Database Not Accessible
**Problem**: SQLite database not being created/saved
**Solution**: Database is created in container filesystem, not mounted volume
```bash
# Access training metrics directly from container
docker exec micro_training sqlite3 /workspace/micro_training.db \
  "SELECT * FROM training_metrics ORDER BY episode_num DESC LIMIT 10"
```

#### Issue 3: MCTS Debug Logs Flooding
**Problem**: Excessive MCTS simulation logs
**Solution**: Adjust logging level
```bash
docker exec micro_training sed -i 's/DEBUG/INFO/g' \
  /workspace/micro/training/stochastic_mcts.py
```

---

## ğŸ”§ Operational Procedures

### Starting the System
```bash
# 1. Build and start all containers
docker compose up -d --build

# 2. Verify all containers running
docker ps --format "table {{.Names}}\t{{.Status}}"

# 3. Monitor training progress
docker logs -f micro_training | grep Episode

# 4. Check validation status
docker logs micro_validation --tail 20
```

### Monitoring Performance
```bash
# Real-time dashboard (from host)
./monitor.sh

# Python dashboard with trade stats
python3 micro/monitoring/advanced_dash.py

# TensorBoard (if needed)
tensorboard --logdir micro/tensorboard --port 6006
```

### Checkpoint Management
```python
# Checkpoints saved every 50 episodes
Location: micro/checkpoints/
Files:
  - latest.pth (most recent)
  - best.pth (highest SQN score)
  - micro_checkpoint_ep{N}.pth (every 50 episodes)

# Only last 5 checkpoints kept to save space
```

### Troubleshooting Guide

| Issue | Diagnosis | Solution |
|-------|-----------|----------|
| Training stuck | Check `docker logs micro_training` | Restart container: `docker restart micro_training` |
| No progress | Check CPU usage: `docker stats` | Increase worker count in code |
| Validation timeout | Check logs for "timed out" | Reduce MC_RUNS in docker-compose |
| High memory usage | `docker stats` shows >90% | Reduce batch size or buffer size |
| Can't find database | Database in container FS | Use docker exec to access |

---

## ğŸš€ Quick Start from Scratch

### Prerequisites
```bash
# System requirements
- Ubuntu 20.04+ or similar Linux
- Docker & Docker Compose
- Python 3.8+
- 16GB+ RAM
- 8+ CPU cores recommended
- 50GB+ free disk space
```

### Installation Steps
```bash
# 1. Clone repository
git clone https://github.com/yourusername/new_swt.git
cd new_swt

# 2. Download OANDA data (if needed)
python3 data/download_oanda_data.py \
  --symbol GBPJPY \
  --start 2022-01-01 \
  --end 2025-08-31

# 3. Prepare feature database
python3 micro/data/prepare_micro_features.py

# 4. Set environment variables
cat > .env << EOF
OANDA_TOKEN=your_token_here
OANDA_ACCOUNT=your_account_here
EOF

# 5. Build and launch
docker compose up -d --build

# 6. Monitor progress
./monitor.sh
```

---

## ğŸ“Š Performance Expectations

### Training Milestones
| Episode | Expected Metrics | Status |
|---------|-----------------|--------|
| 0-1,000 | Random behavior, -5 to -10 pip expectancy | Learning basics |
| 1,000-5,000 | Gradual improvement, -5 to -2 pips | Pattern recognition |
| **5,000-10,000** | **First positive expectancy** | **Break-even point** |
| 10,000-50,000 | +1 to +5 pips expectancy | Refinement |
| 50,000-100,000 | +5 to +10 pips, 40%+ win rate | Professional level |
| 100,000-1,000,000 | +10+ pips, 45%+ win rate | Production ready |

### Computational Requirements
```yaml
Training Phase:
  Duration: ~30 days to 1M episodes
  Speed: ~1,200 episodes/hour
  GPU: Not required (CPU only)
  Storage: ~10GB for checkpoints

Validation:
  Time per validation: 5 minutes (200 MC runs)
  Frequency: On new best checkpoint only

Live Trading:
  Latency: <100ms per decision
  Memory: <2GB
  CPU: <10% single core
```

---

## ğŸ”¬ Technical Deep Dive

### Monte Carlo Tree Search (MCTS) Implementation

#### Tree Node Structures
```python
@dataclass
class DecisionNode:
    """Node where agent chooses action"""
    prior: float                    # Prior probability from parent
    hidden_state: torch.Tensor      # (256,) state representation
    reward: float                   # Immediate reward
    visit_count: int = 0           # Number of visits
    value_sum: float = 0            # Sum of backpropagated values
    children: Dict[int, ChanceNode] # Action â†’ ChanceNode mapping

    def ucb_score(self, c_puct=1.25):
        """Upper Confidence Bound for selection"""
        if self.visit_count == 0:
            return float('inf')
        avg_value = self.value_sum / self.visit_count
        exploration = c_puct * self.prior * sqrt(parent_visits) / (1 + self.visit_count)
        return avg_value + exploration

@dataclass
class ChanceNode:
    """Node representing market uncertainty"""
    prior: float                    # Prior from parent
    action: int                     # Action taken to reach here
    outcome_probabilities: np.ndarray  # [P(UP), P(NEUTRAL), P(DOWN)]
    parent_hidden: torch.Tensor    # Parent's hidden state
    children: Dict[int, DecisionNode]  # Outcome â†’ DecisionNode

    def sample_outcome(self):
        """Sample market outcome based on probabilities"""
        return np.random.choice([0, 1, 2], p=self.outcome_probabilities)
```

#### MCTS Algorithm Flow
```python
1. Selection: Traverse tree using UCB until leaf
2. Expansion: Create chance node for best action
3. Evaluation: Neural network evaluates position
4. Simulation: Sample outcomes and continue
5. Backpropagation: Update all visited nodes
```

### Experience Buffer Implementation

```python
@dataclass
class Experience:
    """Single experience for replay buffer"""
    # Observation can be either format:
    observation: Union[
        np.ndarray,                    # (32, 15) combined format
        Tuple[np.ndarray, np.ndarray]  # ((32, 9), (6,)) separated
    ]
    action: int                        # 0-3 (HOLD/BUY/SELL/CLOSE)
    reward: float                      # Immediate reward
    policy: np.ndarray                 # MCTS policy distribution (4,)
    value: float                       # MCTS value estimate
    done: bool                         # Episode termination flag
    market_outcome: int                # 0=UP, 1=NEUTRAL, 2=DOWN
    outcome_probs: Optional[np.ndarray] # [P(UP), P(NEUTRAL), P(DOWN)]

class ExperienceBuffer:
    """Balanced buffer with quota management"""
    def __init__(self, capacity=10000):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)
        self.trade_quota = 0.30  # Minimum 30% trades
        self.success_memory = deque(maxlen=1000)

    def add(self, experience):
        # Add to success memory if profitable
        if experience.reward > 5.0:
            self.success_memory.append(experience)
        # Maintain trade quota
        if len(self.buffer) >= self.capacity:
            self._evict_with_quota()
        self.buffer.append(experience)
```

### Numba JIT Optimizations
```python
# Performance-critical functions
@numba.jit(nopython=True, parallel=True)
def calculate_market_outcome_numba(
    price_change: float,
    rolling_std: float,
    threshold_multiplier: float = 0.33
) -> int:
    """5-10x faster outcome calculation"""
    threshold = threshold_multiplier * rolling_std
    if price_change > threshold:
        return 0  # UP
    elif price_change < -threshold:
        return 2  # DOWN
    return 1      # NEUTRAL

@numba.jit(nopython=True, parallel=True)
def monte_carlo_simulation_numba(
    episodes: np.ndarray,
    n_simulations: int = 200
) -> Tuple[np.ndarray, np.ndarray]:
    """20-50x faster Monte Carlo with parallel execution"""
    results = np.empty((n_simulations, len(episodes)))
    for i in numba.prange(n_simulations):
        # Bootstrap sample with replacement
        sample = np.random.choice(episodes, len(episodes), replace=True)
        results[i] = np.cumsum(sample)
    return np.mean(results, axis=0), np.std(results, axis=0)
```

---

## ğŸ“ Complete Project Structure

### Directory Tree with Descriptions
```
new_swt/
â”œâ”€â”€ README.md                    # This document - complete manual
â”œâ”€â”€ CLAUDE.md                    # AI assistant instructions
â”œâ”€â”€ docker-compose.yml           # 3-container orchestration
â”œâ”€â”€ Dockerfile.micro            # Container build instructions
â”œâ”€â”€ requirements-cpu.txt        # Python dependencies
â”œâ”€â”€ monitor.sh                  # Bash monitoring script
â”‚
â”œâ”€â”€ data/                       # Market data storage
â”‚   â”œâ”€â”€ GBPJPY_M1_REAL_2022-2025.csv  # 3.5 years OANDA data
â”‚   â”œâ”€â”€ micro_features.duckdb         # Feature database
â”‚   â”œâ”€â”€ download_oanda_data.py        # Data fetcher
â”‚   â””â”€â”€ prepare_micro_features.py     # Feature engineering
â”‚
â”œâ”€â”€ micro/                      # Main system directory
â”‚   â”œâ”€â”€ README.md              # Detailed micro documentation
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                # Neural network definitions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ micro_networks.py  # 5 MuZero networks
â”‚   â”‚   â””â”€â”€ tcn_block.py      # TCN implementation
â”‚   â”‚
â”‚   â”œâ”€â”€ training/              # Training system
â”‚   â”‚   â”œâ”€â”€ train_micro_muzero.py      # Main training script
â”‚   â”‚   â”œâ”€â”€ stochastic_mcts.py         # MCTS with chance nodes
â”‚   â”‚   â”œâ”€â”€ episode_runner.py          # Episode collection
â”‚   â”‚   â”œâ”€â”€ parallel_episode_collector.py  # Multi-process collection
â”‚   â”‚   â”œâ”€â”€ checkpoint_manager.py      # Save/load checkpoints
â”‚   â”‚   â”œâ”€â”€ optimized_cache.py         # DuckDB cache
â”‚   â”‚   â””â”€â”€ archive/                   # Old implementations
â”‚   â”‚
â”‚   â”œâ”€â”€ validation/            # Model validation
â”‚   â”‚   â”œâ”€â”€ validate_micro.py          # Validation logic
â”‚   â”‚   â”œâ”€â”€ validate_micro_watcher.py  # Auto-validation
â”‚   â”‚   â””â”€â”€ validation_results/        # Results storage
â”‚   â”‚
â”‚   â”œâ”€â”€ live/                  # Live trading
â”‚   â”‚   â”œâ”€â”€ trade_micro.py            # Trading execution
â”‚   â”‚   â”œâ”€â”€ micro_feature_builder.py  # Real-time features
â”‚   â”‚   â””â”€â”€ idle_wait.py              # Market hours check
â”‚   â”‚
â”‚   â”œâ”€â”€ monitoring/            # Dashboards
â”‚   â”‚   â”œâ”€â”€ simple_dash.py            # Basic metrics
â”‚   â”‚   â”œâ”€â”€ dashboard.py              # Curses interface
â”‚   â”‚   â”œâ”€â”€ advanced_dash.py          # Trade statistics
â”‚   â”‚   â””â”€â”€ show_stats.py             # Quick stats viewer
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                 # Utilities
â”‚   â”‚   â”œâ”€â”€ numba_optimized.py        # JIT functions
â”‚   â”‚   â”œâ”€â”€ market_outcome_calculator.py  # Outcome logic
â”‚   â”‚   â””â”€â”€ session_index_calculator.py   # Session indexing
â”‚   â”‚
â”‚   â”œâ”€â”€ checkpoints/           # Model checkpoints
â”‚   â”‚   â”œâ”€â”€ latest.pth                # Most recent
â”‚   â”‚   â”œâ”€â”€ best.pth                  # Best performer
â”‚   â”‚   â””â”€â”€ micro_checkpoint_ep*.pth  # Periodic saves
â”‚   â”‚
â”‚   â””â”€â”€ tests/                 # Unit tests
â”‚       â””â”€â”€ test_*.py                 # Component tests
â”‚
â””â”€â”€ precomputed_wst/           # WST features (legacy)
    â””â”€â”€ GBPJPY_WST_*.h5              # Precomputed wavelets
```

---

## ğŸ”„ System Interaction Flow

### Component Communication Diagram
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Docker Host System                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚            micro_training Container                   â”‚   â”‚
â”‚  â”‚                                                       â”‚   â”‚
â”‚  â”‚  train_micro_muzero.py                              â”‚   â”‚
â”‚  â”‚       â”œâ”€â”€ ParallelEpisodeCollector (multiprocess)   â”‚   â”‚
â”‚  â”‚       â”‚     â””â”€â”€ episode_runner.py Ã— N workers      â”‚   â”‚
â”‚  â”‚       â”‚           â”œâ”€â”€ MicroEnvironment             â”‚   â”‚
â”‚  â”‚       â”‚           â””â”€â”€ StochasticMCTS               â”‚   â”‚
â”‚  â”‚       â”œâ”€â”€ MicroStochasticMuZero (networks)        â”‚   â”‚
â”‚  â”‚       â”œâ”€â”€ ExperienceBuffer                        â”‚   â”‚
â”‚  â”‚       â””â”€â”€ CheckpointManager                       â”‚   â”‚
â”‚  â”‚             â””â”€â”€ Saves to: /checkpoints/*.pth      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                 â”‚
â”‚                            â–¼                                 â”‚
â”‚                    [Checkpoint Files]                        â”‚
â”‚                            â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           micro_validation Container                  â”‚   â”‚
â”‚  â”‚                                                       â”‚   â”‚
â”‚  â”‚  validate_micro_watcher.py                          â”‚   â”‚
â”‚  â”‚       â”œâ”€â”€ Monitors: /checkpoints/best.pth           â”‚   â”‚
â”‚  â”‚       â”œâ”€â”€ validate_micro.py                        â”‚   â”‚
â”‚  â”‚       â”‚     â”œâ”€â”€ Loads checkpoint                   â”‚   â”‚
â”‚  â”‚       â”‚     â”œâ”€â”€ Monte Carlo simulation (200 runs)  â”‚   â”‚
â”‚  â”‚       â”‚     â””â”€â”€ Dr. Bandy metrics                  â”‚   â”‚
â”‚  â”‚       â””â”€â”€ Saves: /validation_results/*.json        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚             micro_live Container                      â”‚   â”‚
â”‚  â”‚                                                       â”‚   â”‚
â”‚  â”‚  trade_micro.py                                     â”‚   â”‚
â”‚  â”‚       â”œâ”€â”€ Loads: /checkpoints/best.pth              â”‚   â”‚
â”‚  â”‚       â”œâ”€â”€ OANDA API (live prices)                   â”‚   â”‚
â”‚  â”‚       â”œâ”€â”€ micro_feature_builder.py                 â”‚   â”‚
â”‚  â”‚       â”‚     â””â”€â”€ Real-time feature calculation      â”‚   â”‚
â”‚  â”‚       â”œâ”€â”€ StochasticMCTS (inference only)          â”‚   â”‚
â”‚  â”‚       â””â”€â”€ Order execution via OANDA                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                               â”‚
â”‚  [Shared Volumes]                                            â”‚
â”‚  â”œâ”€â”€ ./micro â†’ /workspace/micro (all containers)            â”‚
â”‚  â”œâ”€â”€ ./data â†’ /workspace/data:ro (read-only)               â”‚
â”‚  â””â”€â”€ ./micro/checkpoints â†’ /workspace/micro/checkpoints     â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Sequence
```
1. Training Flow:
   DuckDB Features â†’ Episode Runner â†’ MCTS â†’ Experience â†’ Buffer â†’ Training â†’ Checkpoint

2. Validation Flow:
   Checkpoint â†’ Model Load â†’ Monte Carlo â†’ Metrics â†’ JSON Report

3. Live Trading Flow:
   OANDA Price â†’ Feature Builder â†’ Model Inference â†’ MCTS â†’ Action â†’ OANDA Order

4. Inter-Container Communication:
   - Via shared filesystem (checkpoints, results)
   - No direct network communication
   - File-based signaling for coordination
```

### Key File Interactions
```python
# Training writes:
micro/checkpoints/latest.pth         # Every episode
micro/checkpoints/best.pth          # When SQN improves
micro/checkpoints/micro_checkpoint_ep{N}.pth  # Every 50 episodes

# Validation reads/writes:
Reads: micro/checkpoints/best.pth
Writes: micro/validation_results/validation_ep{N}.json
Writes: micro/validation_results/best_metrics.json

# Live trading reads:
micro/checkpoints/best.pth          # Model weights
data/micro_features.duckdb          # Historical features
Environment: OANDA_TOKEN, OANDA_ACCOUNT  # API credentials
```

### Process Communication
```
Training Container (6 CPU cores):
â”œâ”€â”€ Main Process
â”œâ”€â”€ Worker Process 1 (Episode collection)
â”œâ”€â”€ Worker Process 2 (Episode collection)
â”œâ”€â”€ Worker Process 3 (Episode collection)
â”œâ”€â”€ Worker Process 4 (Episode collection)
â””â”€â”€ Worker Process 5 (Episode collection)

Each worker:
- Loads checkpoint from disk (avoids pickle issues)
- Runs independent episodes
- Returns experiences via multiprocess.Queue
- No shared CUDA tensors (CPU only)
```

### Adding New Features
1. Always test in `micro/tests/` first
2. Update feature count in all relevant configs
3. Retrain from scratch (no transfer learning)
4. Document in this README

### Performance Optimization Checklist
- [ ] Use Numba JIT for numerical loops
- [ ] Profile with cProfile before optimizing
- [ ] Batch operations where possible
- [ ] Cache repeated calculations
- [ ] Use appropriate data structures

---

## ğŸ“š References

### Key Papers
- "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model" (Schrittwieser et al., 2020)
- "Stochastic MuZero" (Antonoglou et al., 2021)
- "Wavelet Scattering Transform" (Bruna & Mallat, 2013)

### System Design Inspirations
- LightZero framework for MCTS implementation
- Dr. Howard Bandy's position sizing methodology
- Van Tharp's SQN for system quality evaluation

---

## ğŸ” Security & Production Notes

### API Keys
- Store in `.env` file (never commit)
- Use read-only keys for backtesting
- Separate keys for live trading

### Risk Management
- Maximum position: 1 unit
- No leverage in current implementation
- Stop loss: Not implemented (use broker settings)
- Daily loss limit: Configure at broker level

### Deployment Checklist
- [ ] Validate on 3+ years of data
- [ ] Achieve SQN > 2.5
- [ ] Test with paper trading account
- [ ] Implement proper logging
- [ ] Set up monitoring alerts
- [ ] Document trading hours
- [ ] Configure firewall rules
- [ ] Backup checkpoint regularly

---

## ğŸ“ Support & Maintenance

### Common Maintenance Tasks
```bash
# Clear old checkpoints
find micro/checkpoints -mtime +7 -delete

# Backup important checkpoints
rsync -av micro/checkpoints/best.pth backups/

# Reset training
rm -rf micro/checkpoints/*.pth
docker restart micro_training

# Update code without stopping
docker exec micro_training git pull
docker restart micro_training
```

### Monitoring Alerts
```python
# Add to monitoring script
if expectancy < -10:
    send_alert("Training diverging")
if win_rate < 5:
    send_alert("Model collapse detected")
if trade_ratio < 50:
    send_alert("Hold-only behavior")
```

---

## ğŸ”® Future Work & Experimental Research

### Implemented but Not in Production

#### 1. Wavelet Scattering Transform (WST) System
**Status**: Fully implemented, replaced by simpler micro features
- **Implementation**: `swt_features/wst_transform.py` - Complete Kymatio-based WST
- **Features**: 67-dimensional wavelet coefficients (J=6, Q=4)
- **Infrastructure**:
  - Precomputation pipeline (`precompute_wst_coefficients.py`)
  - Memory-efficient loader (`swt_features/precomputed_wst_loader.py`)
  - Caching system (`cache/wst_cache.pkl`)
- **Why Replaced**: 15 micro features achieved similar performance with 10x less computation
- **Directories**: `swt_*` folders contain complete implementation

#### 2. EfficientZero Enhancements
**Status**: Research complete, not integrated
**Location**: `experimental_research/`
- **Self-Supervised Consistency Loss** (`consistency_loss.py`)
  - Improves dynamics model accuracy through temporal consistency
  - Implementation ready for integration
- **Value-Prefix Network** (`value_prefix_network.py`)
  - LSTM-based return prediction from intermediate states
  - Could improve long-term value estimation
- **Off-Policy Correction** (`off_policy_correction.py`)
  - Enhanced value target computation for better sample efficiency
  - Expected 50% reduction in training episodes
- **Gumbel Action Selection** (`gumbel_action_selection.py`)
  - Alternative to UCB for MCTS action selection
  - Better exploration in early training
- **ReZero MCTS** (`rezero_mcts.py`)
  - Optimized tree search with better memory management
  - Concurrent prediction batching

#### 3. Advanced MuZero Components
**Status**: Analyzed, implementation templates available
**Location**: `experimental_research/advanced_muzero_analysis.md`
- **Backward View Reanalyze** (`backward_view_reanalyze.py`)
  - Improves credit assignment in trajectories
  - Better handling of delayed rewards
- **UniZero Concurrent Prediction** (`unizero_concurrent_prediction.py`)
  - Batched neural network inference during MCTS
  - 3-5x speedup potential

### Planned Future Improvements

#### Near-Term (3-6 months)
1. **GPU Acceleration**
   - Mixed precision training with `torch.cuda.amp`
   - Expected 5-10x training speedup
   - CUDA kernels for Numba functions

2. **EfficientZero Integration**
   - Priority: Consistency loss for dynamics model
   - Expected: 50% fewer episodes to convergence
   - Validation: A/B testing against current model

3. **Advanced Position Sizing**
   - Kelly criterion with safety factor
   - Based on rolling 100-trade performance
   - Risk-parity across multiple positions

#### Medium-Term (6-12 months)
1. **Transformer Architecture**
   - Replace TCN with multi-head attention
   - Better long-range dependency modeling
   - Positional encoding for time-series

2. **Multi-Symbol Portfolio**
   - Simultaneous trading of GBPJPY, EURUSD, USDJPY
   - Correlation-aware position management
   - Shared representation learning

3. **Advanced Risk Management**
   - Dynamic stop-loss based on volatility
   - Trailing stops with ATR adjustment
   - Maximum daily/weekly exposure limits

#### Long-Term (12+ months)
1. **Hybrid WST-Micro Features**
   - Combine best of both approaches
   - WST for regime detection
   - Micro features for execution

2. **Meta-Learning**
   - Adapt to changing market regimes
   - Few-shot learning for new symbols
   - Online adaptation without retraining

3. **Distributed Training**
   - Multi-GPU data parallel training
   - Distributed MCTS with Ray
   - Cloud-native deployment

---

**Last Updated**: September 29, 2025 | **Version**: 3.1.0 | **Status**: PPO Training at 27,400+ trades | **Architecture**: PPO (Active)

*This document serves as the complete technical and operational manual for the trading systems. The PPO implementation in `/micro/nano/picco-ppo/` is the current active system, with the MuZero implementation preserved for reference.*