# üöÄ Micro Stochastic MuZero Trading System - Complete Technical & Operational Manual
**Version 3.0 | Last Updated: September 22, 2025 | Episode: 4,732**

## üìä System Overview

A **production-grade Stochastic MuZero** implementation for ultra-high-frequency forex trading (GBPJPY 1-minute bars) using 15 carefully selected features with separated temporal/static architecture. The **Micro variant is the main production model** moving forward. This manual provides complete specifications to rebuild and operate the system from scratch.

### Core Innovations
- **Separated Architecture**: Temporal (32√ó9) and static (1√ó6) pathways (PRODUCTION)
- **Stochastic MCTS**: Chance nodes model market uncertainty
- **3 Discrete Outcomes**: UP/NEUTRAL/DOWN based on 0.33œÉ threshold
- **TCN Integration**: 240-channel temporal convolutional network
- **Numba Optimization**: 20-50x speedup on critical paths
- **Three-Container Production**: Training, validation, and live trading separation

---

## üèóÔ∏è Complete System Architecture

### 1. Neural Network Specifications

#### 1.1 Model Architecture: Stochastic MuZero with TCN
```python
# Five Core Networks
1. Representation Network (observation ‚Üí hidden state)
   - Input: Temporal (32, 9) + Static (6,)
   - TCN Block: 9 ‚Üí 240 channels (3 residual blocks)
   - Static MLP: 6 ‚Üí 16 dimensions
   - Output: 256-dimensional hidden state

2. Dynamics Network (state transition)
   - Input: hidden_state (256) + action (4) + outcome (3)
   - Architecture: 3 MLPResidual blocks
   - Output: next_state (256) + reward prediction

3. Policy Network (action prediction)
   - Input: hidden_state (256)
   - Architecture: 2 MLPResidual blocks ‚Üí 4 actions
   - Output: action_logits with temperature scaling

4. Value Network (state evaluation)
   - Input: hidden_state (256)
   - Support: [-300, +300] pips (601 bins)
   - Output: categorical distribution over value bins

5. Afterstate Network (deterministic transition)
   - Input: hidden_state (256) + action (4)
   - Output: afterstate (256) before stochastic outcome
```

#### 1.2 TCN (Temporal Convolutional Network) Block
```python
class TCNBlock:
    in_channels: 9 (temporal features)
    out_channels: 240
    kernel_size: 3
    num_layers: 3
    dilation: [1, 2, 4]  # Exponential receptive field
    dropout: 0.1
    activation: ReLU
    batch_norm: True
```

#### 1.3 Training Hyperparameters
```python
# Core Training Config
learning_rate: 0.002 (fixed, no decay)
batch_size: 64
buffer_size: 10,000
num_simulations: 5 (MCTS per step)
episode_length: 360 bars (6 hours)
target_episodes: 1,000,000
checkpoint_interval: 50 episodes

# Optimizer
optimizer: Adam
weight_decay: 1e-5
gradient_clip: 1.0

# Exploration
epsilon: 0.2 (epsilon-greedy)
temperature: 1.0 ‚Üí 0.5 (decay over 20k episodes)
dirichlet_alpha: 1.0
dirichlet_fraction: 0.5

# Loss Weights
value_loss_weight: 0.25
policy_loss_weight: 1.0
reward_loss_weight: 1.0
outcome_loss_weight: 1.0
```

### 2. Feature Engineering Pipeline

#### 2.1 Input Features (15 total dimensions)
```python
# Temporal Features (32 timesteps √ó 9 features)
Market Features (indices 0-4):
  0: position_in_range_60     # (close - min60) / (max60 - min60)
  1: min_max_scaled_momentum_60  # 60-bar momentum, normalized [0,1]
  2: min_max_scaled_rolling_range  # 60-bar volatility, normalized
  3: min_max_scaled_momentum_5   # 5-bar momentum, normalized
  4: price_change_pips         # tanh((close[t] - close[t-1]) * 100 / 10)

Time Features (indices 5-8):
  5: dow_cos_final   # cos(2œÄ √ó day_of_week / 5)
  6: dow_sin_final   # sin(2œÄ √ó day_of_week / 5)
  7: hour_cos_final  # cos(2œÄ √ó hour / 120) for 120hr trading week
  8: hour_sin_final  # sin(2œÄ √ó hour / 120)

# Static Features (1 timestep √ó 6 features)
Position Features (indices 9-14):
  9:  position_side      # -1 (short), 0 (flat), 1 (long)
  10: position_pips      # tanh(current_pips / 100)
  11: bars_since_entry   # tanh(bars / 100)
  12: pips_from_peak     # tanh(drawdown_pips / 100)
  13: max_drawdown_pips  # tanh(max_dd / 100)
  14: accumulated_dd     # tanh(area_under_dd / 100)
```

#### 2.2 Market Outcome Discretization
```python
# Three discrete outcomes based on rolling œÉ
rolling_std = 20-period standard deviation
outcome_threshold = 0.33 * rolling_std

Outcomes:
  0: DOWN  (price_change < -threshold)
  1: NEUTRAL (-threshold ‚â§ price_change ‚â§ threshold)
  2: UP    (price_change > threshold)
```

### 3. Data Pipeline

#### 3.1 Data Sources
```yaml
Primary Data:
  File: data/GBPJPY_M1_REAL_2022-2025.csv
  Format: OHLCV 1-minute bars
  Period: Jan 2022 - Aug 2025 (3.59 years)
  Total Bars: 1,333,912
  Quality Metrics:
    - Mean pip range: 3.87 pips/min
    - 99th percentile: 16.47 pips
    - Coverage: 94.9% of trading hours

Feature Database:
  File: data/micro_features.duckdb
  Tables: features_micro
  Columns: 15 (as specified above)
  Indexing: bar_index (primary key)
```

#### 3.2 Session Sampling
```python
# Valid Trading Session Criteria
session_length: 360 bars (6 hours)
max_gap: 10 minutes
exclude_weekends: True
min_liquidity: 1.0 pip range

# Pre-indexing at startup
valid_sessions = []
for i in range(len(data) - 360):
    if validate_session(data[i:i+360]):
        valid_sessions.append(i)
# Result: ~5,000 valid sessions from 1.33M bars
```

### 4. Trading Environment

#### 4.1 Action Space
```python
Actions = {
    0: HOLD,   # No position change
    1: BUY,    # Enter long position
    2: SELL,   # Enter short position
    3: CLOSE   # Exit current position
}

# Position constraints
max_position: 1 unit
position_type: net (no hedging)
```

#### 4.2 Reward System
```python
# AMDDP1 (Average Maximum Drawdown with 1% Penalty)
def calculate_reward(action, position, pnl):
    if action in [BUY, SELL]:
        return +1.0  # Entry reward
    elif action == HOLD:
        if position == 0:
            return -0.05  # Idle penalty
        else:
            return 0.0   # In-trade hold
    elif action == CLOSE:
        amddp = pnl - 0.01 * max_drawdown
        return amddp
```

#### 4.3 Transaction Costs
```python
spread: 4 pips (fixed)
commission: 0
slippage: 0 (using limit orders)
margin: Not modeled (assumed sufficient)
```

---

## üê≥ Docker Deployment Architecture

### Container Specifications

```yaml
# docker-compose.yml structure
version: '3.8'

services:
  micro-training:
    image: micro-muzero:latest
    container_name: micro_training
    resources:
      memory: 8GB
      cpus: 6.0
    volumes:
      - ./micro:/workspace/micro
      - ./data:/workspace/data:ro
      - ./micro/checkpoints:/workspace/micro/checkpoints
    environment:
      - PYTHONPATH=/workspace
      - OMP_NUM_THREADS=1
    command: python3 /workspace/micro/training/train_micro_muzero.py

  micro-validation:
    image: micro-muzero:latest
    container_name: micro_validation
    resources:
      memory: 4GB
      cpus: 2.0
    volumes:
      - ./micro:/workspace/micro
      - ./data:/workspace/data:ro
      - ./micro/checkpoints:/workspace/micro/checkpoints
      - ./micro/validation_results:/workspace/micro/validation_results
    command: python3 /workspace/micro/validation/validate_micro_watcher.py
    environment:
      - MC_RUNS=200  # Reduced from 1000
      - VALIDATION_TIMEOUT=1800

  micro-live:
    image: micro-muzero:latest
    container_name: micro_live
    resources:
      memory: 2GB
      cpus: 1.0
    volumes:
      - ./micro:/workspace/micro
      - ./data:/workspace/data:ro
      - ./micro/checkpoints:/workspace/micro/checkpoints
    command: python3 /workspace/micro/live/trade_live.py
    environment:
      - OANDA_TOKEN=${OANDA_TOKEN}
      - OANDA_ACCOUNT=${OANDA_ACCOUNT}
```

---

## üìà Current Operational Status (As of September 22, 2025)

### Training Progress
```
Episode: 4,732 / 1,000,000 (0.47% complete)
Training Speed: ~1,200 episodes/hour
Time Running: 4+ hours
Next Milestone: Episode 5,000 (ETA: 13 minutes)

Performance Metrics:
- Expectancy: -4.06 pips (still negative)
- Win Rate: 9.4%
- Trade Ratio: 75.3% (excellent engagement)
- Loss: 88.59 (decreasing slowly)

Action Distribution:
- HOLD: 25%
- BUY: 23%
- SELL: 24%
- CLOSE: 25%
```

### Known Issues & Solutions

#### Issue 1: Validation Timeout
**Problem**: Validation times out after 10 minutes with 1000 Monte Carlo runs
**Solution**: Already fixed in code but container needs restart
```bash
# Fix: Restart validation with new settings
docker restart micro_validation
```

#### Issue 2: Training Database Not Accessible
**Problem**: SQLite database not being created/saved
**Solution**: Database is created in container filesystem, not mounted volume
```bash
# Access training metrics directly from container
docker exec micro_training sqlite3 /workspace/micro_training.db \
  "SELECT * FROM training_metrics ORDER BY episode_num DESC LIMIT 10"
```

#### Issue 3: MCTS Debug Logs Flooding
**Problem**: Excessive MCTS simulation logs
**Solution**: Adjust logging level
```bash
docker exec micro_training sed -i 's/DEBUG/INFO/g' \
  /workspace/micro/training/stochastic_mcts.py
```

---

## üîß Operational Procedures

### Starting the System
```bash
# 1. Build and start all containers
docker compose up -d --build

# 2. Verify all containers running
docker ps --format "table {{.Names}}\t{{.Status}}"

# 3. Monitor training progress
docker logs -f micro_training | grep Episode

# 4. Check validation status
docker logs micro_validation --tail 20
```

### Monitoring Performance
```bash
# Real-time dashboard (from host)
./monitor.sh

# Python dashboard with trade stats
python3 micro/monitoring/advanced_dash.py

# TensorBoard (if needed)
tensorboard --logdir micro/tensorboard --port 6006
```

### Checkpoint Management
```python
# Checkpoints saved every 50 episodes
Location: micro/checkpoints/
Files:
  - latest.pth (most recent)
  - best.pth (highest SQN score)
  - micro_checkpoint_ep{N}.pth (every 50 episodes)

# Only last 5 checkpoints kept to save space
```

### Troubleshooting Guide

| Issue | Diagnosis | Solution |
|-------|-----------|----------|
| Training stuck | Check `docker logs micro_training` | Restart container: `docker restart micro_training` |
| No progress | Check CPU usage: `docker stats` | Increase worker count in code |
| Validation timeout | Check logs for "timed out" | Reduce MC_RUNS in docker-compose |
| High memory usage | `docker stats` shows >90% | Reduce batch size or buffer size |
| Can't find database | Database in container FS | Use docker exec to access |

---

## üöÄ Quick Start from Scratch

### Prerequisites
```bash
# System requirements
- Ubuntu 20.04+ or similar Linux
- Docker & Docker Compose
- Python 3.8+
- 16GB+ RAM
- 8+ CPU cores recommended
- 50GB+ free disk space
```

### Installation Steps
```bash
# 1. Clone repository
git clone https://github.com/yourusername/new_swt.git
cd new_swt

# 2. Download OANDA data (if needed)
python3 data/download_oanda_data.py \
  --symbol GBPJPY \
  --start 2022-01-01 \
  --end 2025-08-31

# 3. Prepare feature database
python3 micro/data/prepare_micro_features.py

# 4. Set environment variables
cat > .env << EOF
OANDA_TOKEN=your_token_here
OANDA_ACCOUNT=your_account_here
EOF

# 5. Build and launch
docker compose up -d --build

# 6. Monitor progress
./monitor.sh
```

---

## üìä Performance Expectations

### Training Milestones
| Episode | Expected Metrics | Status |
|---------|-----------------|--------|
| 0-1,000 | Random behavior, -5 to -10 pip expectancy | Learning basics |
| 1,000-5,000 | Gradual improvement, -5 to -2 pips | Pattern recognition |
| **5,000-10,000** | **First positive expectancy** | **Break-even point** |
| 10,000-50,000 | +1 to +5 pips expectancy | Refinement |
| 50,000-100,000 | +5 to +10 pips, 40%+ win rate | Professional level |
| 100,000-1,000,000 | +10+ pips, 45%+ win rate | Production ready |

### Computational Requirements
```yaml
Training Phase:
  Duration: ~30 days to 1M episodes
  Speed: ~1,200 episodes/hour
  GPU: Not required (CPU only)
  Storage: ~10GB for checkpoints

Validation:
  Time per validation: 5 minutes (200 MC runs)
  Frequency: On new best checkpoint only

Live Trading:
  Latency: <100ms per decision
  Memory: <2GB
  CPU: <10% single core
```

---

## üî¨ Technical Deep Dive

### Monte Carlo Tree Search (MCTS) Implementation

#### Tree Node Structures
```python
@dataclass
class DecisionNode:
    """Node where agent chooses action"""
    prior: float                    # Prior probability from parent
    hidden_state: torch.Tensor      # (256,) state representation
    reward: float                   # Immediate reward
    visit_count: int = 0           # Number of visits
    value_sum: float = 0            # Sum of backpropagated values
    children: Dict[int, ChanceNode] # Action ‚Üí ChanceNode mapping

    def ucb_score(self, c_puct=1.25):
        """Upper Confidence Bound for selection"""
        if self.visit_count == 0:
            return float('inf')
        avg_value = self.value_sum / self.visit_count
        exploration = c_puct * self.prior * sqrt(parent_visits) / (1 + self.visit_count)
        return avg_value + exploration

@dataclass
class ChanceNode:
    """Node representing market uncertainty"""
    prior: float                    # Prior from parent
    action: int                     # Action taken to reach here
    outcome_probabilities: np.ndarray  # [P(UP), P(NEUTRAL), P(DOWN)]
    parent_hidden: torch.Tensor    # Parent's hidden state
    children: Dict[int, DecisionNode]  # Outcome ‚Üí DecisionNode

    def sample_outcome(self):
        """Sample market outcome based on probabilities"""
        return np.random.choice([0, 1, 2], p=self.outcome_probabilities)
```

#### MCTS Algorithm Flow
```python
1. Selection: Traverse tree using UCB until leaf
2. Expansion: Create chance node for best action
3. Evaluation: Neural network evaluates position
4. Simulation: Sample outcomes and continue
5. Backpropagation: Update all visited nodes
```

### Experience Buffer Implementation

```python
@dataclass
class Experience:
    """Single experience for replay buffer"""
    # Observation can be either format:
    observation: Union[
        np.ndarray,                    # (32, 15) combined format
        Tuple[np.ndarray, np.ndarray]  # ((32, 9), (6,)) separated
    ]
    action: int                        # 0-3 (HOLD/BUY/SELL/CLOSE)
    reward: float                      # Immediate reward
    policy: np.ndarray                 # MCTS policy distribution (4,)
    value: float                       # MCTS value estimate
    done: bool                         # Episode termination flag
    market_outcome: int                # 0=UP, 1=NEUTRAL, 2=DOWN
    outcome_probs: Optional[np.ndarray] # [P(UP), P(NEUTRAL), P(DOWN)]

class ExperienceBuffer:
    """Balanced buffer with quota management"""
    def __init__(self, capacity=10000):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)
        self.trade_quota = 0.30  # Minimum 30% trades
        self.success_memory = deque(maxlen=1000)

    def add(self, experience):
        # Add to success memory if profitable
        if experience.reward > 5.0:
            self.success_memory.append(experience)
        # Maintain trade quota
        if len(self.buffer) >= self.capacity:
            self._evict_with_quota()
        self.buffer.append(experience)
```

### Numba JIT Optimizations
```python
# Performance-critical functions
@numba.jit(nopython=True, parallel=True)
def calculate_market_outcome_numba(
    price_change: float,
    rolling_std: float,
    threshold_multiplier: float = 0.33
) -> int:
    """5-10x faster outcome calculation"""
    threshold = threshold_multiplier * rolling_std
    if price_change > threshold:
        return 0  # UP
    elif price_change < -threshold:
        return 2  # DOWN
    return 1      # NEUTRAL

@numba.jit(nopython=True, parallel=True)
def monte_carlo_simulation_numba(
    episodes: np.ndarray,
    n_simulations: int = 200
) -> Tuple[np.ndarray, np.ndarray]:
    """20-50x faster Monte Carlo with parallel execution"""
    results = np.empty((n_simulations, len(episodes)))
    for i in numba.prange(n_simulations):
        # Bootstrap sample with replacement
        sample = np.random.choice(episodes, len(episodes), replace=True)
        results[i] = np.cumsum(sample)
    return np.mean(results, axis=0), np.std(results, axis=0)
```

---

## üìÅ Complete Project Structure

### Directory Tree with Descriptions
```
new_swt/
‚îú‚îÄ‚îÄ README.md                    # This document - complete manual
‚îú‚îÄ‚îÄ CLAUDE.md                    # AI assistant instructions
‚îú‚îÄ‚îÄ docker-compose.yml           # 3-container orchestration
‚îú‚îÄ‚îÄ Dockerfile.micro            # Container build instructions
‚îú‚îÄ‚îÄ requirements-cpu.txt        # Python dependencies
‚îú‚îÄ‚îÄ monitor.sh                  # Bash monitoring script
‚îÇ
‚îú‚îÄ‚îÄ data/                       # Market data storage
‚îÇ   ‚îú‚îÄ‚îÄ GBPJPY_M1_REAL_2022-2025.csv  # 3.5 years OANDA data
‚îÇ   ‚îú‚îÄ‚îÄ micro_features.duckdb         # Feature database
‚îÇ   ‚îú‚îÄ‚îÄ download_oanda_data.py        # Data fetcher
‚îÇ   ‚îî‚îÄ‚îÄ prepare_micro_features.py     # Feature engineering
‚îÇ
‚îú‚îÄ‚îÄ micro/                      # Main system directory
‚îÇ   ‚îú‚îÄ‚îÄ README.md              # Detailed micro documentation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                # Neural network definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ micro_networks.py  # 5 MuZero networks
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tcn_block.py      # TCN implementation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ training/              # Training system
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_micro_muzero.py      # Main training script
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stochastic_mcts.py         # MCTS with chance nodes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ episode_runner.py          # Episode collection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parallel_episode_collector.py  # Multi-process collection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ checkpoint_manager.py      # Save/load checkpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ optimized_cache.py         # DuckDB cache
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ archive/                   # Old implementations
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ validation/            # Model validation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validate_micro.py          # Validation logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validate_micro_watcher.py  # Auto-validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validation_results/        # Results storage
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ live/                  # Live trading
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trade_micro.py            # Trading execution
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ micro_feature_builder.py  # Real-time features
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ idle_wait.py              # Market hours check
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/            # Dashboards
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ simple_dash.py            # Basic metrics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dashboard.py              # Curses interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ advanced_dash.py          # Trade statistics
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ show_stats.py             # Quick stats viewer
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                 # Utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ numba_optimized.py        # JIT functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ market_outcome_calculator.py  # Outcome logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ session_index_calculator.py   # Session indexing
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/           # Model checkpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ latest.pth                # Most recent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ best.pth                  # Best performer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ micro_checkpoint_ep*.pth  # Periodic saves
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ tests/                 # Unit tests
‚îÇ       ‚îî‚îÄ‚îÄ test_*.py                 # Component tests
‚îÇ
‚îî‚îÄ‚îÄ precomputed_wst/           # WST features (legacy)
    ‚îî‚îÄ‚îÄ GBPJPY_WST_*.h5              # Precomputed wavelets
```

---

## üîÑ System Interaction Flow

### Component Communication Diagram
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Docker Host System                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ            micro_training Container                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  train_micro_muzero.py                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îú‚îÄ‚îÄ ParallelEpisodeCollector (multiprocess)   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ     ‚îî‚îÄ‚îÄ episode_runner.py √ó N workers      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ           ‚îú‚îÄ‚îÄ MicroEnvironment             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ           ‚îî‚îÄ‚îÄ StochasticMCTS               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îú‚îÄ‚îÄ MicroStochasticMuZero (networks)        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îú‚îÄ‚îÄ ExperienceBuffer                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îî‚îÄ‚îÄ CheckpointManager                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ             ‚îî‚îÄ‚îÄ Saves to: /checkpoints/*.pth      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                            ‚îÇ                                 ‚îÇ
‚îÇ                            ‚ñº                                 ‚îÇ
‚îÇ                    [Checkpoint Files]                        ‚îÇ
‚îÇ                            ‚îÇ                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ           micro_validation Container                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  validate_micro_watcher.py                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îú‚îÄ‚îÄ Monitors: /checkpoints/best.pth           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îú‚îÄ‚îÄ validate_micro.py                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ     ‚îú‚îÄ‚îÄ Loads checkpoint                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ     ‚îú‚îÄ‚îÄ Monte Carlo simulation (200 runs)  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ     ‚îî‚îÄ‚îÄ Dr. Bandy metrics                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îî‚îÄ‚îÄ Saves: /validation_results/*.json        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ             micro_live Container                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  trade_micro.py                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îú‚îÄ‚îÄ Loads: /checkpoints/best.pth              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îú‚îÄ‚îÄ OANDA API (live prices)                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îú‚îÄ‚îÄ micro_feature_builder.py                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ     ‚îî‚îÄ‚îÄ Real-time feature calculation      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îú‚îÄ‚îÄ StochasticMCTS (inference only)          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       ‚îî‚îÄ‚îÄ Order execution via OANDA                ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  [Shared Volumes]                                            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ ./micro ‚Üí /workspace/micro (all containers)            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ ./data ‚Üí /workspace/data:ro (read-only)               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ ./micro/checkpoints ‚Üí /workspace/micro/checkpoints     ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow Sequence
```
1. Training Flow:
   DuckDB Features ‚Üí Episode Runner ‚Üí MCTS ‚Üí Experience ‚Üí Buffer ‚Üí Training ‚Üí Checkpoint

2. Validation Flow:
   Checkpoint ‚Üí Model Load ‚Üí Monte Carlo ‚Üí Metrics ‚Üí JSON Report

3. Live Trading Flow:
   OANDA Price ‚Üí Feature Builder ‚Üí Model Inference ‚Üí MCTS ‚Üí Action ‚Üí OANDA Order

4. Inter-Container Communication:
   - Via shared filesystem (checkpoints, results)
   - No direct network communication
   - File-based signaling for coordination
```

### Key File Interactions
```python
# Training writes:
micro/checkpoints/latest.pth         # Every episode
micro/checkpoints/best.pth          # When SQN improves
micro/checkpoints/micro_checkpoint_ep{N}.pth  # Every 50 episodes

# Validation reads/writes:
Reads: micro/checkpoints/best.pth
Writes: micro/validation_results/validation_ep{N}.json
Writes: micro/validation_results/best_metrics.json

# Live trading reads:
micro/checkpoints/best.pth          # Model weights
data/micro_features.duckdb          # Historical features
Environment: OANDA_TOKEN, OANDA_ACCOUNT  # API credentials
```

### Process Communication
```
Training Container (6 CPU cores):
‚îú‚îÄ‚îÄ Main Process
‚îú‚îÄ‚îÄ Worker Process 1 (Episode collection)
‚îú‚îÄ‚îÄ Worker Process 2 (Episode collection)
‚îú‚îÄ‚îÄ Worker Process 3 (Episode collection)
‚îú‚îÄ‚îÄ Worker Process 4 (Episode collection)
‚îî‚îÄ‚îÄ Worker Process 5 (Episode collection)

Each worker:
- Loads checkpoint from disk (avoids pickle issues)
- Runs independent episodes
- Returns experiences via multiprocess.Queue
- No shared CUDA tensors (CPU only)
```

### Adding New Features
1. Always test in `micro/tests/` first
2. Update feature count in all relevant configs
3. Retrain from scratch (no transfer learning)
4. Document in this README

### Performance Optimization Checklist
- [ ] Use Numba JIT for numerical loops
- [ ] Profile with cProfile before optimizing
- [ ] Batch operations where possible
- [ ] Cache repeated calculations
- [ ] Use appropriate data structures

---

## üìö References

### Key Papers
- "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model" (Schrittwieser et al., 2020)
- "Stochastic MuZero" (Antonoglou et al., 2021)
- "Wavelet Scattering Transform" (Bruna & Mallat, 2013)

### System Design Inspirations
- LightZero framework for MCTS implementation
- Dr. Howard Bandy's position sizing methodology
- Van Tharp's SQN for system quality evaluation

---

## üîê Security & Production Notes

### API Keys
- Store in `.env` file (never commit)
- Use read-only keys for backtesting
- Separate keys for live trading

### Risk Management
- Maximum position: 1 unit
- No leverage in current implementation
- Stop loss: Not implemented (use broker settings)
- Daily loss limit: Configure at broker level

### Deployment Checklist
- [ ] Validate on 3+ years of data
- [ ] Achieve SQN > 2.5
- [ ] Test with paper trading account
- [ ] Implement proper logging
- [ ] Set up monitoring alerts
- [ ] Document trading hours
- [ ] Configure firewall rules
- [ ] Backup checkpoint regularly

---

## üìû Support & Maintenance

### Common Maintenance Tasks
```bash
# Clear old checkpoints
find micro/checkpoints -mtime +7 -delete

# Backup important checkpoints
rsync -av micro/checkpoints/best.pth backups/

# Reset training
rm -rf micro/checkpoints/*.pth
docker restart micro_training

# Update code without stopping
docker exec micro_training git pull
docker restart micro_training
```

### Monitoring Alerts
```python
# Add to monitoring script
if expectancy < -10:
    send_alert("Training diverging")
if win_rate < 5:
    send_alert("Model collapse detected")
if trade_ratio < 50:
    send_alert("Hold-only behavior")
```

---

## üîÆ Future Work & Experimental Research

### Implemented but Not in Production

#### 1. Wavelet Scattering Transform (WST) System
**Status**: Fully implemented, replaced by simpler micro features
- **Implementation**: `swt_features/wst_transform.py` - Complete Kymatio-based WST
- **Features**: 67-dimensional wavelet coefficients (J=6, Q=4)
- **Infrastructure**:
  - Precomputation pipeline (`precompute_wst_coefficients.py`)
  - Memory-efficient loader (`swt_features/precomputed_wst_loader.py`)
  - Caching system (`cache/wst_cache.pkl`)
- **Why Replaced**: 15 micro features achieved similar performance with 10x less computation
- **Directories**: `swt_*` folders contain complete implementation

#### 2. EfficientZero Enhancements
**Status**: Research complete, not integrated
**Location**: `experimental_research/`
- **Self-Supervised Consistency Loss** (`consistency_loss.py`)
  - Improves dynamics model accuracy through temporal consistency
  - Implementation ready for integration
- **Value-Prefix Network** (`value_prefix_network.py`)
  - LSTM-based return prediction from intermediate states
  - Could improve long-term value estimation
- **Off-Policy Correction** (`off_policy_correction.py`)
  - Enhanced value target computation for better sample efficiency
  - Expected 50% reduction in training episodes
- **Gumbel Action Selection** (`gumbel_action_selection.py`)
  - Alternative to UCB for MCTS action selection
  - Better exploration in early training
- **ReZero MCTS** (`rezero_mcts.py`)
  - Optimized tree search with better memory management
  - Concurrent prediction batching

#### 3. Advanced MuZero Components
**Status**: Analyzed, implementation templates available
**Location**: `experimental_research/advanced_muzero_analysis.md`
- **Backward View Reanalyze** (`backward_view_reanalyze.py`)
  - Improves credit assignment in trajectories
  - Better handling of delayed rewards
- **UniZero Concurrent Prediction** (`unizero_concurrent_prediction.py`)
  - Batched neural network inference during MCTS
  - 3-5x speedup potential

### Planned Future Improvements

#### Near-Term (3-6 months)
1. **GPU Acceleration**
   - Mixed precision training with `torch.cuda.amp`
   - Expected 5-10x training speedup
   - CUDA kernels for Numba functions

2. **EfficientZero Integration**
   - Priority: Consistency loss for dynamics model
   - Expected: 50% fewer episodes to convergence
   - Validation: A/B testing against current model

3. **Advanced Position Sizing**
   - Kelly criterion with safety factor
   - Based on rolling 100-trade performance
   - Risk-parity across multiple positions

#### Medium-Term (6-12 months)
1. **Transformer Architecture**
   - Replace TCN with multi-head attention
   - Better long-range dependency modeling
   - Positional encoding for time-series

2. **Multi-Symbol Portfolio**
   - Simultaneous trading of GBPJPY, EURUSD, USDJPY
   - Correlation-aware position management
   - Shared representation learning

3. **Advanced Risk Management**
   - Dynamic stop-loss based on volatility
   - Trailing stops with ATR adjustment
   - Maximum daily/weekly exposure limits

#### Long-Term (12+ months)
1. **Hybrid WST-Micro Features**
   - Combine best of both approaches
   - WST for regime detection
   - Micro features for execution

2. **Meta-Learning**
   - Adapt to changing market regimes
   - Few-shot learning for new symbols
   - Online adaptation without retraining

3. **Distributed Training**
   - Multi-GPU data parallel training
   - Distributed MCTS with Ray
   - Cloud-native deployment

---

**Last Updated**: September 22, 2025 | **Version**: 3.0.0 | **Status**: Training at Episode 4,732 | **Architecture**: Micro (Production)

*This document serves as the complete technical and operational manual for rebuilding and operating the Micro Stochastic MuZero trading system from scratch. The Micro variant with separated temporal/static architecture is the production model moving forward.*